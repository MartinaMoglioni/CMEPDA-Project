{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WaveletDNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/federicocampo/CMEPDA-Project/blob/Prime_modifiche/WaveletDNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7UQdhtmCS3B"
      },
      "source": [
        "Cloning into our repository to import all the needed functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNBucPrt99Aa",
        "outputId": "71374e1b-a370-4810-bfd0-619db9b68135"
      },
      "source": [
        "!git clone https://github.com/federicocampo/CMEPDA-Project.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CMEPDA-Project'...\n",
            "remote: Enumerating objects: 232, done.\u001b[K\n",
            "remote: Counting objects: 100% (232/232), done.\u001b[K\n",
            "remote: Compressing objects: 100% (208/208), done.\u001b[K\n",
            "remote: Total 232 (delta 90), reused 50 (delta 21), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (232/232), 3.62 MiB | 9.66 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNVOhk1rMi4D",
        "outputId": "70e0868a-3430-400b-e9ef-8f68470aa50e"
      },
      "source": [
        "%cd /content/CMEPDA-Project/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CMEPDA-Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIPzsClBDhdW",
        "outputId": "0299c1e3-7ae0-49b3-c5f5-d3b29c2db0db"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOV3UF99AhGP"
      },
      "source": [
        "import os\r\n",
        "from PIL import Image\r\n",
        "import pywt\r\n",
        "from skimage.restoration import denoise_wavelet\r\n",
        "from skimage import img_as_float\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import glob\r\n",
        "import multiprocessing as mp\r\n",
        "\r\n",
        "from keras.layers import BatchNormalization, Dense, Flatten, Activation, Dropout, Input\r\n",
        "from keras.models import Sequential, load_model\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from keras.optimizers import SGD, Adam\r\n",
        "\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "\r\n",
        "from waveletconverter import dwtcoefftoarray\r\n",
        "\r\n",
        "import logging\r\n",
        "\r\n",
        "#Setting a logger and a logging level\r\n",
        "logger = logging.getLogger('Mylogger')\r\n",
        "logger.setLevel(logging.DEBUG)\r\n",
        "#Setting an handler to send logging output\r\n",
        "ch = logging.StreamHandler()\r\n",
        "ch.setLevel(logging.DEBUG)\r\n",
        "#Set the format of every log message printing the name of logger, logging level and the message.\r\n",
        "formatter = logging.Formatter('%(name)s (%(levelname)s): %(message)s')\r\n",
        "ch.setFormatter(formatter)\r\n",
        "#Add the specified handler to this logger.\r\n",
        "logger.addHandler(ch)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUJCHbedvlEc"
      },
      "source": [
        "#Read the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRLV3NwMEW-v",
        "outputId": "65699abf-dd1f-4160-ee49-dff200d9ef5b"
      },
      "source": [
        "PATH = '/content/gdrive/My Drive/IMAGES/Mammography_micro'\r\n",
        "\r\n",
        "def read_img(image_path):\r\n",
        "  ''' Takes as input the path to the image folder and \r\n",
        "      returns the numpy array of images and label found in that folder\r\n",
        "  '''\r\n",
        "\r\n",
        "  # Creating a list of all image names found in image_path\r\n",
        "  imagefilename = glob.glob(os.path.join(image_path, '*.pgm'))\r\n",
        "\r\n",
        "  # Defining 4 sub-processes and apply imread to all the images found previously\r\n",
        "  # (imread reads images in pgm format)\r\n",
        "  pool = mp.Pool(processes=4)\r\n",
        "  results = pool.map_async(Image.open, imagefilename)\r\n",
        "\r\n",
        "  # Get the list of images and convert to numpy array\r\n",
        "  images = results.get()\r\n",
        "\r\n",
        "  logger.info(f'Num images found in {image_path}: {len(images)}')\r\n",
        "\r\n",
        "  # Create a list of corrisponding labels and conver it to numpy array\r\n",
        "  label = os.path.basename(image_path)\r\n",
        "  y = [int(label)] * len(images)\r\n",
        "  y_np = np.array(y)\r\n",
        "\r\n",
        "  return images, y_np\r\n",
        "\r\n",
        "\r\n",
        "# Define the path to the sub-folder of Train images folder containing \"normal\" breast mammograms\r\n",
        "image_path = os.path.join(PATH, 'Train/0')\r\n",
        "# Create the test images and labels array with read_img function\r\n",
        "images0_train, y0_train = read_img(image_path)\r\n",
        "\r\n",
        "# Define the path to the sub-folder of Train images folder containing \"normal\" breast mammograms\r\n",
        "image_path = os.path.join(PATH, 'Train/1')\r\n",
        "# Create the test images and labels array with read_img function\r\n",
        "images1_train, y1_train = read_img(image_path)\r\n",
        "\r\n",
        "# Create an array with both normal and microcalcifications containing images and labels\r\n",
        "images_train = images0_train + images1_train\r\n",
        "y_train = np.concatenate((y0_train, y1_train))\r\n",
        "\r\n",
        "# Define the path to the sub-folder of Train images folder containing \"normal\" breast mammograms\r\n",
        "image_path = os.path.join(PATH, 'Test/0')\r\n",
        "# Create the test images and labels array with read_img function\r\n",
        "images0_test, y0_test = read_img(image_path)\r\n",
        "\r\n",
        "# Define the path to the sub-folder of Train images folder containing \"normal\" breast mammograms\r\n",
        "image_path = os.path.join(PATH, 'Test/1')\r\n",
        "# Create the test images and labels array with read_img function\r\n",
        "images1_test, y1_test = read_img(image_path)\r\n",
        "\r\n",
        "# Create an array with both normal and microcalcifications containing images and labels\r\n",
        "images_test = images0_test + images1_test\r\n",
        "y_test = np.concatenate((y0_test, y1_test))\r\n",
        "\r\n",
        "images_tot = images_test + images_train\r\n",
        "labels = np.concatenate((y_test, y_train))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mylogger (INFO): Num images found in /content/gdrive/My Drive/IMAGES/Mammography_micro/Train/0: 209\n",
            "Mylogger (INFO): Num images found in /content/gdrive/My Drive/IMAGES/Mammography_micro/Train/0: 209\n",
            "Mylogger (INFO): Num images found in /content/gdrive/My Drive/IMAGES/Mammography_micro/Train/1: 187\n",
            "Mylogger (INFO): Num images found in /content/gdrive/My Drive/IMAGES/Mammography_micro/Train/1: 187\n",
            "Mylogger (INFO): Num images found in /content/gdrive/My Drive/IMAGES/Mammography_micro/Test/0: 205\n",
            "Mylogger (INFO): Num images found in /content/gdrive/My Drive/IMAGES/Mammography_micro/Test/0: 205\n",
            "Mylogger (INFO): Num images found in /content/gdrive/My Drive/IMAGES/Mammography_micro/Test/1: 196\n",
            "Mylogger (INFO): Num images found in /content/gdrive/My Drive/IMAGES/Mammography_micro/Test/1: 196\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkW1aJuAKIdu"
      },
      "source": [
        "wavelet = 'db5'\r\n",
        "level = 4\r\n",
        "denoise = 'no'\r\n",
        "\r\n",
        "coefficients = []\r\n",
        "\r\n",
        "for i, image in enumerate(images_tot):\r\n",
        "  array = dwtcoefftoarray(images_tot[i], wavelet, level, denoise)\r\n",
        "  coefficients.append(array)\r\n",
        "\r\n",
        "coefficients = np.array(coefficients)\r\n",
        "\r\n",
        "print(coefficients.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lep8LG7MvowQ"
      },
      "source": [
        "#Define and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeHKcX-6ThO4"
      },
      "source": [
        "def make_model():\r\n",
        "  model = Sequential([\r\n",
        "                      Input(shape=(6042)),\r\n",
        "                      Dense(10, activation='relu'),\r\n",
        "                      Dense(20, activation='relu'),\r\n",
        "                      Dense(20, activation='relu'),\r\n",
        "                      Dense(10, activation='relu'),\r\n",
        "                      #Dense(50, activation = 'relu'),\r\n",
        "                      Dense(1, activation='sigmoid')\r\n",
        "  ])\r\n",
        "  return model"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl3JXHtpU8Mf"
      },
      "source": [
        "#Set a checkpoint to save weights giving the best performance on val_accuracy \r\n",
        "checkpoint = ModelCheckpoint(\r\n",
        "    \"model-{epoch:02d}-{val_accuracy:.2f}.hdf5\", \r\n",
        "    monitor='val_accuracy', \r\n",
        "    verbose=1,\r\n",
        "    save_best_only=True,\r\n",
        "    save_weights_only=False,\r\n",
        "    mode='auto', save_freq='epoch')"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huSEQkpOWzvs"
      },
      "source": [
        "''' Define the model and compile it\r\n",
        "'''\r\n",
        "model = make_model()\r\n",
        "model.compile(optimizer = Adam(lr = 0.005), metrics = 'accuracy', loss='binary_crossentropy')"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mIXdD3WyyLi"
      },
      "source": [
        "##Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6UwZw-6XECa",
        "outputId": "1fe0d4cc-03e4-4fd5-9319-6a55913bcee7"
      },
      "source": [
        "''' Training the model using x_train and y_train images and labels and doing a validation split of 30%\r\n",
        "'''\r\n",
        "\r\n",
        "val_split = 0.3\r\n",
        "\r\n",
        "history = model.fit(coefficients, labels, \r\n",
        "                    validation_split=val_split, \r\n",
        "                    epochs=100, \r\n",
        "                    batch_size=30, \r\n",
        "                    shuffle=True, \r\n",
        "                    callbacks = [checkpoint],\r\n",
        "                    verbose=1)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "19/19 [==============================] - 1s 20ms/step - loss: 24.2803 - accuracy: 0.5336 - val_loss: 0.6365 - val_accuracy: 0.3417\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.34167, saving model to model-01-0.34.hdf5\n",
            "Epoch 2/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.7174 - accuracy: 0.6645 - val_loss: 0.7058 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.34167\n",
            "Epoch 3/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6762 - accuracy: 0.6518 - val_loss: 0.7193 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.34167\n",
            "Epoch 4/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6685 - accuracy: 0.6617 - val_loss: 0.7342 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.34167\n",
            "Epoch 5/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6675 - accuracy: 0.6390 - val_loss: 0.6868 - val_accuracy: 0.3208\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.34167\n",
            "Epoch 6/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6632 - accuracy: 0.6448 - val_loss: 0.7609 - val_accuracy: 0.2250\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.34167\n",
            "Epoch 7/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6442 - accuracy: 0.6724 - val_loss: 0.7770 - val_accuracy: 0.2250\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.34167\n",
            "Epoch 8/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6224 - accuracy: 0.7208 - val_loss: 0.7998 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.34167\n",
            "Epoch 9/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6549 - accuracy: 0.6548 - val_loss: 0.8117 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.34167\n",
            "Epoch 10/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6563 - accuracy: 0.6455 - val_loss: 0.8219 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.34167\n",
            "Epoch 11/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6565 - accuracy: 0.6412 - val_loss: 0.8326 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.34167\n",
            "Epoch 12/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6647 - accuracy: 0.6197 - val_loss: 0.8401 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.34167\n",
            "Epoch 13/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6369 - accuracy: 0.6815 - val_loss: 0.8526 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.34167\n",
            "Epoch 14/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6560 - accuracy: 0.6370 - val_loss: 0.8579 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.34167\n",
            "Epoch 15/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6650 - accuracy: 0.6176 - val_loss: 0.8666 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.34167\n",
            "Epoch 16/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6517 - accuracy: 0.6444 - val_loss: 0.8723 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.34167\n",
            "Epoch 17/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6381 - accuracy: 0.6700 - val_loss: 0.8799 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.34167\n",
            "Epoch 18/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6395 - accuracy: 0.6659 - val_loss: 0.8846 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.34167\n",
            "Epoch 19/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6391 - accuracy: 0.6661 - val_loss: 0.8889 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.34167\n",
            "Epoch 20/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6527 - accuracy: 0.6413 - val_loss: 0.8914 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.34167\n",
            "Epoch 21/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6585 - accuracy: 0.6310 - val_loss: 0.8950 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.34167\n",
            "Epoch 22/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6497 - accuracy: 0.6465 - val_loss: 0.8971 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.34167\n",
            "Epoch 23/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6457 - accuracy: 0.6532 - val_loss: 0.9001 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.34167\n",
            "Epoch 24/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6339 - accuracy: 0.6733 - val_loss: 0.9028 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.34167\n",
            "Epoch 25/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6515 - accuracy: 0.6433 - val_loss: 0.9026 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.34167\n",
            "Epoch 26/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6532 - accuracy: 0.6404 - val_loss: 0.9026 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.34167\n",
            "Epoch 27/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6419 - accuracy: 0.6594 - val_loss: 0.9041 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.34167\n",
            "Epoch 28/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6381 - accuracy: 0.6658 - val_loss: 0.9051 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.34167\n",
            "Epoch 29/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6688 - accuracy: 0.6145 - val_loss: 0.9017 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.34167\n",
            "Epoch 30/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6578 - accuracy: 0.6327 - val_loss: 0.9013 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.34167\n",
            "Epoch 31/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6406 - accuracy: 0.6618 - val_loss: 0.9057 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.34167\n",
            "Epoch 32/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6426 - accuracy: 0.6581 - val_loss: 0.9050 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.34167\n",
            "Epoch 33/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6496 - accuracy: 0.6466 - val_loss: 0.9060 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.34167\n",
            "Epoch 34/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6523 - accuracy: 0.6420 - val_loss: 0.9063 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.34167\n",
            "Epoch 35/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6552 - accuracy: 0.6373 - val_loss: 0.9064 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.34167\n",
            "Epoch 36/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6408 - accuracy: 0.6610 - val_loss: 0.9099 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.34167\n",
            "Epoch 37/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6420 - accuracy: 0.6589 - val_loss: 0.9127 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.34167\n",
            "Epoch 38/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6435 - accuracy: 0.6564 - val_loss: 0.9104 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.34167\n",
            "Epoch 39/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6452 - accuracy: 0.6537 - val_loss: 0.9097 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.34167\n",
            "Epoch 40/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6553 - accuracy: 0.6371 - val_loss: 0.9087 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.34167\n",
            "Epoch 41/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6569 - accuracy: 0.6345 - val_loss: 0.9087 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.34167\n",
            "Epoch 42/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6490 - accuracy: 0.6475 - val_loss: 0.9075 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.34167\n",
            "Epoch 43/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6639 - accuracy: 0.6229 - val_loss: 0.9076 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.34167\n",
            "Epoch 44/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6380 - accuracy: 0.6658 - val_loss: 0.9115 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.34167\n",
            "Epoch 45/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6387 - accuracy: 0.6643 - val_loss: 0.9138 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.34167\n",
            "Epoch 46/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6337 - accuracy: 0.6723 - val_loss: 0.9118 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.34167\n",
            "Epoch 47/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6487 - accuracy: 0.6481 - val_loss: 0.9101 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.34167\n",
            "Epoch 48/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6284 - accuracy: 0.6811 - val_loss: 0.9122 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.34167\n",
            "Epoch 49/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6653 - accuracy: 0.6212 - val_loss: 0.9058 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.34167\n",
            "Epoch 50/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6323 - accuracy: 0.6753 - val_loss: 0.9086 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.34167\n",
            "Epoch 51/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6411 - accuracy: 0.6606 - val_loss: 0.9071 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.34167\n",
            "Epoch 52/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6378 - accuracy: 0.6661 - val_loss: 0.9107 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.34167\n",
            "Epoch 53/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6241 - accuracy: 0.6881 - val_loss: 0.9144 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.34167\n",
            "Epoch 54/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6387 - accuracy: 0.6641 - val_loss: 0.9093 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.34167\n",
            "Epoch 55/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6357 - accuracy: 0.6694 - val_loss: 0.9092 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.34167\n",
            "Epoch 56/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6501 - accuracy: 0.6457 - val_loss: 0.9099 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.34167\n",
            "Epoch 57/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6327 - accuracy: 0.6741 - val_loss: 0.9121 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.34167\n",
            "Epoch 58/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6431 - accuracy: 0.6570 - val_loss: 0.9076 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.34167\n",
            "Epoch 59/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6556 - accuracy: 0.6365 - val_loss: 0.9063 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.34167\n",
            "Epoch 60/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6555 - accuracy: 0.6367 - val_loss: 0.9063 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.34167\n",
            "Epoch 61/100\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.6489 - accuracy: 0.6476 - val_loss: 0.9131 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.34167\n",
            "Epoch 62/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6459 - accuracy: 0.6525 - val_loss: 0.9096 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.34167\n",
            "Epoch 63/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6617 - accuracy: 0.6268 - val_loss: 0.9084 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.34167\n",
            "Epoch 64/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6474 - accuracy: 0.6502 - val_loss: 0.9081 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.34167\n",
            "Epoch 65/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6451 - accuracy: 0.6540 - val_loss: 0.9104 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.34167\n",
            "Epoch 66/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6528 - accuracy: 0.6412 - val_loss: 0.9101 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.34167\n",
            "Epoch 67/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6327 - accuracy: 0.6741 - val_loss: 0.9135 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.34167\n",
            "Epoch 68/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6492 - accuracy: 0.6472 - val_loss: 0.9112 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.34167\n",
            "Epoch 69/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6463 - accuracy: 0.6520 - val_loss: 0.9125 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.34167\n",
            "Epoch 70/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6652 - accuracy: 0.6212 - val_loss: 0.9080 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.34167\n",
            "Epoch 71/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6299 - accuracy: 0.6790 - val_loss: 0.9124 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.34167\n",
            "Epoch 72/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6487 - accuracy: 0.6481 - val_loss: 0.9117 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.34167\n",
            "Epoch 73/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6542 - accuracy: 0.6390 - val_loss: 0.9087 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.34167\n",
            "Epoch 74/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6631 - accuracy: 0.6243 - val_loss: 0.9096 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.34167\n",
            "Epoch 75/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6442 - accuracy: 0.6554 - val_loss: 0.9104 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.34167\n",
            "Epoch 76/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6615 - accuracy: 0.6271 - val_loss: 0.9077 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.34167\n",
            "Epoch 77/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6372 - accuracy: 0.6671 - val_loss: 0.9105 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.34167\n",
            "Epoch 78/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6511 - accuracy: 0.6440 - val_loss: 0.9095 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.34167\n",
            "Epoch 79/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6589 - accuracy: 0.6313 - val_loss: 0.9118 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.34167\n",
            "Epoch 80/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6705 - accuracy: 0.6125 - val_loss: 0.9055 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.34167\n",
            "Epoch 81/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6395 - accuracy: 0.6634 - val_loss: 0.9119 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.34167\n",
            "Epoch 82/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6522 - accuracy: 0.6423 - val_loss: 0.9120 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.34167\n",
            "Epoch 83/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6591 - accuracy: 0.6311 - val_loss: 0.9086 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.34167\n",
            "Epoch 84/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6405 - accuracy: 0.6616 - val_loss: 0.9130 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.34167\n",
            "Epoch 85/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.6636 - val_loss: 0.9138 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.34167\n",
            "Epoch 86/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6528 - accuracy: 0.6414 - val_loss: 0.9094 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.34167\n",
            "Epoch 87/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6409 - accuracy: 0.6608 - val_loss: 0.9099 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.34167\n",
            "Epoch 88/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6381 - accuracy: 0.6653 - val_loss: 0.9093 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.34167\n",
            "Epoch 89/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6454 - accuracy: 0.6535 - val_loss: 0.9079 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.34167\n",
            "Epoch 90/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6492 - accuracy: 0.6472 - val_loss: 0.9036 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.34167\n",
            "Epoch 91/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6484 - accuracy: 0.6485 - val_loss: 0.9054 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.34167\n",
            "Epoch 92/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6271 - accuracy: 0.6839 - val_loss: 0.9085 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.34167\n",
            "Epoch 93/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6446 - accuracy: 0.6547 - val_loss: 0.9069 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.34167\n",
            "Epoch 94/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6352 - accuracy: 0.6705 - val_loss: 0.9089 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.34167\n",
            "Epoch 95/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6535 - accuracy: 0.6401 - val_loss: 0.9059 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.34167\n",
            "Epoch 96/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6663 - accuracy: 0.6188 - val_loss: 0.9050 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.34167\n",
            "Epoch 97/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6603 - accuracy: 0.6288 - val_loss: 0.9079 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.34167\n",
            "Epoch 98/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6366 - accuracy: 0.6680 - val_loss: 0.9113 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.34167\n",
            "Epoch 99/100\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.6580 - accuracy: 0.6328 - val_loss: 0.9106 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.34167\n",
            "Epoch 100/100\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6558 - accuracy: 0.6364 - val_loss: 0.9089 - val_accuracy: 0.2208\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.34167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "g1gXhyMOaLiu",
        "outputId": "d0dcf257-4249-4293-b0d6-43c279df8b0b"
      },
      "source": [
        "''' Visualize loss, val_loss, accuracy and val_accuracy obtanined during the train\r\n",
        "''' \r\n",
        "plt.figure(1)\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('Loss and val_loss')\r\n",
        "\r\n",
        "plt.figure(2)\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('Accuracy and val_accuracy')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy and val_accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbgUlEQVR4nO3de5wdZZ3n8c83nUsnIbcmjVwSSIDAGHG42CKMFxjF3SDDZcfRCSrC6sjLC4qOsw6ODquMOy+dnRVQWWcYRHAEI6JilomicllHRiCNsEBAICCQxAQa0p3EXOju9G//qDrkpHNOn+rkNJ16+vt+vc6rT1U9p+qpruR7nn6q6ilFBGZmVn7jRrsCZmbWHA50M7NEONDNzBLhQDczS4QD3cwsEQ50M7NEONDNcpKeknTKCKw3JB3e7PWaDeZAT5ikOyR1S5o02nUxs5HnQE+UpHnAG4EAzniZtz3+5dye7TkfszQ40NP1XuAu4Brg3OoFkuZK+oGkLkkvSPpa1bIPSHpE0iZJD0s6Lp+/U7eBpGskfSF/f7Kk1ZL+WtI64JuSZkm6Od9Gd/5+TtXn2yR9U9Lv8uU35fMfknR6VbkJkp6XdOzgHSywjTsk/Z2kO/P9+amk2VXLz5H0dP47+Ey9X6Sk10laJ6mlat5/kfRA/v54Sb+S1CNpraSvSZo45NHZdRunSbpP0kZJqyR9btDyN0j6j3wbqySdl8+fLOl/5fuxQdIv83knS1o9aB0vdSlJ+pykGyV9W9JG4LxG+yHpVZJ+Jmm9pGcl/Y2k/SVtkbRvVbnj8mMyYTi/A9tzDvR0vRe4Ln/9Z0mvAMhD6WbgaWAecBCwJF/2DuBz+Wenk7XsXyi4vf2BNuAQ4Hyyf1vfzKcPBrYCX6sq/6/AFOBVwH7Apfn8bwHvqSr3NmBtRNxXY5uNtgHwLuC/5tuYCPxVvq8Lga8D5wAHAvsCc6ghIu4GNgNvHrTe6/P324FPALOBE4G3AB+uta4hbCb7vc8ETgM+JOmsvK6HAD8Gvgq0A8cA9+ef+0fgNcAfkf3+PwUMFNzmmcCN+TavG2o/JE0Dfg78hOz3dThwa0SsA+4A3lm13nOAJRHRN4z9t2aICL8SewFvAPqA2fn0b4BP5O9PBLqA8TU+dwtwYZ11BnB41fQ1wBfy9ycDvUDrEHU6BujO3x9AFjqzapQ7ENgETM+nbwQ+VXC/X9pGPn0H8Nmq6Q8DP8nfX0wWOpVlU/N9OKXOur8AXJ2/n0YWwIfUKftx4If1fncF9+Uy4NL8/aer11dVZhzZl9jRNZadDKweNO+pyv6RfXH/okEdXtoP4Gzgvjrl/hy4M3/fAqwDjh/t/wdj8eUWeprOBX4aEc/n09ezo9tlLvB0RPTX+Nxc4Ind3GZXRGyrTEiaIumf866AjcAvgJn5XwhzgfUR0T14JRHxO+BO4O2SZgKnkrUed9FgGxXrqt5vAfbJ3x8IrKra7maG/mvkeuBP8xPMfwr8OiKezutxRN7dsy6vx9+TtXILy7t1bs+7KjYAH6xaR73jMhtorbOsiFXVEw32Y6h/Gz8CFkqaD7wV2BAR9+xmnWwPONATI2ky2Z+/J+X/MdeR/Rl9tKSjyf4TH6zaJ8FWAYfVWfUWsi6Siv0HLR88bOcngSOB10XEdOBNlSrm22nLA7uWa8m6Xd4B/Coi1tQpN9Q2GllLFlLZB6QpZN0uNUXEw2TdVKeyc3cLZF03vwEW5PX4m4J1qHY9sBSYGxEzgH+qWke94/I8sK3Oss1UHa/8S659UJnBx2yo/VgFHFqr4vkX+Q1kx+wcsu40GwUO9PScRdYXupCsC+IY4JXAv5P10d5DFmZflDRVUquk1+efvQr4K0mvUebwvP8Wsj7bd0lqkbQIOKlBPaaRdQf0SGoD/ntlQUSsJesT/t/5ic0Jkt5U9dmbgOOAC8n61Ie9jQJuBP4kP9k4EbiExv8frs/r9Cbge4PqsRH4vaQ/AD40jHpUr2N9RGyTdDzZl0bFdcApkt4pabykfSUdExEDwNXAlyUdmB+bE/O/Ih4DWvOTrROAzwKNLl8daj9uBg6Q9HFJkyRNk/S6quXfAs4jO+/iQB8lDvT0nAt8MyKeiYh1lRfZycJ3k7W4Tic7qfUMsJqsD5SI+B7wP8iCaxNZsLbl670w/1xPvp6bGtTjMmAyWSvyLrKTadXOIevn/w3wHFl/LXk9tgLfB+YDP9iDbdQVESuAj5Dt61qgm+x3MZTvkH2R3VbVnQXZidZ3kf3O/gX4btF6VPkwcImkTWT9+zdU1fUZspPDnwTWk325Hl217QeB5fmyLwHjImJDvs6rgDVkLfZG+1d3PyJiE1l3yulk3ViPA39ctfxOsvMiL3VF2ctP+YkMs72KpIuBIyLiPQ0L215B0m3A9RFx1WjXZazyzQS218m7T95P1oq3EpD0WrJusjNHuy5jmbtcbK8i6QNkJ+B+HBG/GO36NJOkFZJ+X+P17tGu256QdC3ZNeofz7tmbJQU6nLJT4JdTnaN6VUR8cVByy9lR3/aFGC/iKh3BYOZmY2AhoGeX+70GNkJkdVkJ1/Ozi/jqlX+o8CxEfG+JtfVzMyGUKQP/XhgZUQ8CSBpCVk/Wc1AJ7ujrOHlY7Nnz4558+YVrKaZmQHce++9z0fE4HsKgGKBfhA731G2GnhdrYL5NcvzgdvqLD+fbJwPDj74YDo7Owts3szMKiTVvSy02SdFFwM3RsT2Wgsj4sqI6IiIjvb2ml8wZma2m4oE+hqqbpEmG5Gu3q3Yi8luvjAzs5dZkUBfDiyQND+/RXox2ZgTO8lvFZ4F/Kq5VTQzsyIaBno+Kt8FZEOrPgLcEBErJF0iqfpJOIvJhiP1radmZqOg0J2iEbEMWDZo3sWDpj/XvGqZmdlw+U5RM7NEONDNzBIx5gN9+0DwwOoerrnzt/z2+c2jXR0zs902Zkdb3D4QfOaHD/JvD65l07bsaWxnrerhssW7PFzezKwUxmygL3twLUuWr+L0ow/klFfuxw/vW8Pyp3Z5xKWZWWmUOtC39PbT+VQ3b1wwG6n4IxwHBoKv3vY4C/bbh8v//BjGjRMv/L6XOx7tYu2GrRwwY/II1trMbGSUug/95gfW8t6r7+H/PtY1rM/9+KF1PPbs7/noWxYwblz2RfDaedmT1jrdSjezkip1oHdtehGAy299nKL3Mw0MBF+59XEOa5/Kaa8+4KX5rzxgGlMmttD51PoRqauZ2UgrdaBv2NoHwH3P9PDvjz9fs0xEcO/T3Ty7cRsAt6xYx6PPbuKjb15Ay7gd3TTjW8Zx7MEz3Y9uZqVV6j707s29zN5nEhNbxOW3Pl6zL/3bdz/D3970EACHzp7Klt7tHDp7KqcffeAu6+s4pI2v3vY4m7b1Ma11wsuyD2ZmzVLqFnr3lj7ap03iQycfxr1Pd3Pnyhd2Wt751Ho+v3QFJx3RzmdPeyXzZ0+lf2CATy06cqfWecVr57UxEFmL38ysbErdQu/Z0susKRN452vncsXtT3DZzx/jhEPbGN8yjmc3buND1/2aObMm85Wzj2XG5An8xRsPHXJ9xxw8k3HKvgjedITHazezcil1oHdv6eXI/acxaXwLH/njw/jbH63g6M//lNfOb+O5jS/y+239fPv9r2PG5GLdJ/tMGs/CA6e7H93MSqnUgb5hax8zp0wE4D0nHEL7tFZ+ubKLu55cz2+f38zli4/hyP2nDWudHYe0sWT5M/RtH2BCS6l7pMxsjCltoEcEPVv6mDUla31LYtFR+7PoqP0B6O0fYOL44Qdyx7xZXPMfT7Hidxs5Zu7MptbZzGwklTbQN73YT/9AMCtvoQ+2O2EOWQsd4LvLn+F3PVt3u35mZvW86sDpHLLv1Kavt7SB3rM5uwZ9Zp1A3137z2jl8P324Tv3rOI796xq6rrNzAC+cNZRDvRq3Vt6AZhZ8ITncHz/Q3/Eug3bmr5eMzOAV0yfNCLrLW2g9+R3ic6a2vxAnzF5QuErY8zM9halvYyjp9JCb3KXi5lZWZU20Ls3Z4Fe76SomdlYU95A35J1ubhrxMwsU9pA79nSy/TW8TXHZDEzG4vKG+hb+5g11d0tZmYVpQ307i19PiFqZlaltIFeGWnRzMwypQ307i29vsLFzKxKaQO9Z3MfM91CNzN7SSkDvW/7AJte7GfmZLfQzcwqShnoG0bwtn8zs7IqZaD7tn8zs12VMtArd4n6Khczsx3KGegex8XMbBelDPTK0Lkex8XMbIdCgS5pkaRHJa2UdFGdMu+U9LCkFZKub241d1bpQ/et/2ZmOzR8wIWkFuAK4K3AamC5pKUR8XBVmQXAp4HXR0S3pP1GqsKQ9aFPaBFTJ7aM5GbMzEqlSAv9eGBlRDwZEb3AEuDMQWU+AFwREd0AEfFcc6u5s54tvcycMhHJIy2amVUUCfSDgOqnJa/O51U7AjhC0p2S7pK0qNaKJJ0vqVNSZ1dX1+7VGOje3OcrXMzMBmnWSdHxwALgZOBs4F8kzRxcKCKujIiOiOhob2/f7Y31bO31XaJmZoMUCfQ1wNyq6Tn5vGqrgaUR0RcRvwUeIwv4EdGzxeO4mJkNViTQlwMLJM2XNBFYDCwdVOYmstY5kmaTdcE82cR67sQjLZqZ7aphoEdEP3ABcAvwCHBDRKyQdImkM/JitwAvSHoYuB34bxHxwkhUOCKyh1t4HBczs500vGwRICKWAcsGzbu46n0Af5m/RtTWvu309g+4hW5mNkjp7hTtycdxmem7RM3MdlK6QO/2SItmZjWVLtB7PNKimVlNpQv0bo/jYmZWUwkDPe9DdwvdzGwnpQv03v4BJraM852iZmaDFLpscW/y/jfM532vn+eBuczMBildCx1wmJuZ1VDKQDczs1050M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MElEo0CUtkvSopJWSLqqx/DxJXZLuz19/0fyqmpnZUMY3KiCpBbgCeCuwGlguaWlEPDyo6Hcj4oIRqKOZmRVQpIV+PLAyIp6MiF5gCXDmyFbLzMyGq0igHwSsqppenc8b7O2SHpB0o6S5tVYk6XxJnZI6u7q6dqO6ZmZWT7NOiv4fYF5E/CHwM+DaWoUi4sqI6IiIjvb29iZt2szMoFigrwGqW9xz8nkviYgXIuLFfPIq4DXNqZ6ZmRVVJNCXAwskzZc0EVgMLK0uIOmAqskzgEeaV0UzMyui4VUuEdEv6QLgFqAFuDoiVki6BOiMiKXAxySdAfQD64HzRrDOZmZWgyJiVDbc0dERnZ2do7JtM7OyknRvRHTUWuY7Rc3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLhAPdzCwRDnQzs0Q40M3MEuFANzNLRKFAl7RI0qOSVkq6aIhyb5cUkjqaV0UzMyuiYaBLagGuAE4FFgJnS1pYo9w04ELg7mZX0szMGivSQj8eWBkRT0ZEL7AEOLNGub8DvgRsa2L9zMysoCKBfhCwqmp6dT7vJZKOA+ZGxL81sW5mZjYMe3xSVNI44MvAJwuUPV9Sp6TOrq6uPd20mZlVKRLoa4C5VdNz8nkV04CjgDskPQWcACytdWI0Iq6MiI6I6Ghvb9/9WpuZ2S6KBPpyYIGk+ZImAouBpZWFEbEhImZHxLyImAfcBZwREZ0jUmMzM6upYaBHRD9wAXAL8AhwQ0SskHSJpDNGuoJmZlbM+CKFImIZsGzQvIvrlD15z6tlZmbD5TtFzcwS4UA3M0uEA93MLBEOdDOzRDjQzcwS4UA3M0uEA93MLBEOdDOzRDjQzcwS4UA3M0uEA93MLBEOdDOzRDjQzcwS4UA3M0uEA93MLBEOdDOzRDjQzcwS4UA3M0tEOQO9v3e0a2BmttcpX6D/8lL4Qjv0vzjaNTEz26uUL9An7pP93LZxdOthZraXKV+gT5qe/XzRgW5mVq18gd6aB/q2ntGth5nZXqZ8gV5pobvLxcxsJ+UL9FZ3uZiZ1VK+QHcL3cyspvIFeuuM7Kdb6GZmOylfoE+alv10C93MbCflC/RxLTBxWvEW+pb1cOlRsObXI1svM7NRVr5Ah+zEaNEW+gtPwIZVsPb+ka2TmdkoK2egT5oOL24oVnZrd/Zzy/qRq4+Z2V6gnIE+nBZ6JdArP83MElXOQJ80vXgfulvoZjZGlDPQW6fDtmF2uWx1oJtZ2soZ6JOG0+WSB7lb6GaWuEKBLmmRpEclrZR0UY3lH5T0oKT7Jf1S0sLmV7VK6250ubiFbmaJaxjoklqAK4BTgYXA2TUC+/qIeHVEHAP8A/Dlpte02qTpsL0X+rY1Lus+dDMbI4q00I8HVkbEkxHRCywBzqwuEBHVzeWpQDSvijUM5/b/SqBv64GBgZGrk5nZKCsS6AcBq6qmV+fzdiLpI5KeIGuhf6zWiiSdL6lTUmdXV9fu1DcznAG6Ki3zGPAY6maWtKadFI2IKyLiMOCvgc/WKXNlRHREREd7e/vub+ylFnqBK122du/4AvC16GaWsCKBvgaYWzU9J59XzxLgrD2pVEOtBVvoA9uzyxv3PSybdj+6mSWsSKAvBxZImi9pIrAYWFpdQNKCqsnTgMebV8Uaij5XdNsGIKAtD3Rf6WJmCRvfqEBE9Eu6ALgFaAGujogVki4BOiNiKXCBpFOAPqAbOHckK124hV7pYnEL3czGgIaBDhARy4Blg+ZdXPX+wibXa2hFW+iVQHcL3czGgJLeKVp5yEWDk6KVQJ81DzTOLXQzS1o5A73ykIuiXS5T9oXWmW6hm1nSyhnoUOz2/0qLfEpb9nIL3cwSVt5An1RgxMVKC711BkxucwvdzJJW3kAv0kLf2p2F+biWvIXuG4vMLF3lDfQiQ+hu7YbJs7L3bqGbWeLKG+iFWujrdwS6+9DNLHElDvQZBVvobdn7KW3QvxX6to583czMRkF5A73Ic0UHd7mAW+lmlqzyBnprgYdcVAf6lDzQ3Y9uZokqb6A3uv1/YDts7XEL3czGjPIGemVM9HrXoldGWpxS1YcObqGbWbLKG+iNnlpUuanILXQzGyPKG+iVIXTrPbVoa/64Ofehm9kYUd5Ab9hCz4O7EujjJ8GEqb5b1MySVd5Ab21wUnRwlwtkrXS30M0sUeUN9MJ96G075k2e5T50M0tWiQM9f8hFoxZ65WoYcAvdzJJW3kBv9JCLrd0waQa0VD1lb7LHczGzdJU30GHoAbq2rIfJM3ee5xa6mSWs5IE+o/6NRdW3/VdMbssuZxzYPvJ1MzN7mZU70IcaoGtr945rzyumtAHR+ElHZmYlVO5Abx3iMXT1WujgfnQzS1K5A32opxZVP9yiwneLmlnCyh3o9U6KDgzsPNJihVvoZpaw8Y2L7MUqLfQIkHbMfzEfaXGXFno+fedlsOKHoHHwytPhyFN3/ryZWQmVO9Bbp8NAH/RvgwmTd8yvdZcowPQ5cPCJsPF32at3M/y/6+Gwt8CiL0L7ES9f3c3MmqzcgV65/f+f3pjdaFTRnz/FaPB16OMnwvt+smN6ex8s/wbc/vfw9ROh7TC31M1s5J30KTjq7U1fbbkDfcF/gle/I3sU3WCHvB4OPmHoz7dMgBM+CK/+M/jlpbBh1cjU08ysWuvMxmV2gyJiRFbcSEdHR3R2do7Kts3MykrSvRHRUWtZua9yMTOzlzjQzcwS4UA3M0uEA93MLBGFAl3SIkmPSlop6aIay/9S0sOSHpB0q6RDml9VMzMbSsNAl9QCXAGcCiwEzpa0cFCx+4COiPhD4EbgH5pdUTMzG1qRFvrxwMqIeDIieoElwJnVBSLi9ojYkk/eBcxpbjXNzKyRIoF+EFB9x83qfF497wd+XGuBpPMldUrq7OrqKl5LMzNrqKl3ikp6D9ABnFRreURcCVyZl+2S9PRubmo28PxufrbMxuJ+j8V9hrG532Nxn2H4+133HGWRQF8DzK2anpPP24mkU4DPACdFxIuNVhoR7QW2XZOkznp3SqVsLO73WNxnGJv7PRb3GZq730W6XJYDCyTNlzQRWAwsHVShY4F/Bs6IiOeaUTEzMxuehoEeEf3ABcAtwCPADRGxQtIlks7Ii/1PYB/ge5Lul7S0zurMzGyEFOpDj4hlwLJB8y6uen9Kk+vVyJUv8/b2FmNxv8fiPsPY3O+xuM/QxP0etdEWzcysuXzrv5lZIhzoZmaJKF2gNxpXJgWS5kq6PR8fZ4WkC/P5bZJ+Junx/OesRusqG0ktku6TdHM+PV/S3fnx/m5+pVVSJM2UdKOk30h6RNKJY+RYfyL/9/2QpO9Iak3teEu6WtJzkh6qmlfz2CrzlXzfH5B03HC3V6pALziuTAr6gU9GxELgBOAj+X5eBNwaEQuAW/Pp1FxIdjVVxZeASyPicKCb7E7k1FwO/CQi/gA4mmz/kz7Wkg4CPkY2BtRRQAvZJdGpHe9rgEWD5tU7tqcCC/LX+cDXh7uxUgU6BcaVSUFErI2IX+fvN5H9Bz+IbF+vzYtdC5w1OjUcGZLmAKcBV+XTAt5MNuAbpLnPM4A3Ad8AiIjeiOgh8WOdGw9MljQemAKsJbHjHRG/ANYPml3v2J4JfCsydwEzJR0wnO2VLdCHO65M6UmaBxwL3A28IiLW5ovWAa8YpWqNlMuATwED+fS+QE9+LwSkebznA13AN/OupqskTSXxYx0Ra4B/BJ4hC/INwL2kf7yh/rHd43wrW6CPKZL2Ab4PfDwiNlYvi+x602SuOZX0J8BzEXHvaNflZTYeOA74ekQcC2xmUPdKascaIO83PpPsC+1AYCq7dk0kr9nHtmyBXmhcmRRImkAW5tdFxA/y2c9W/gTLf6Y0zMLrgTMkPUXWlfZmsr7lmfmf5JDm8V4NrI6Iu/PpG8kCPuVjDXAK8NuI6IqIPuAHZP8GUj/eUP/Y7nG+lS3QG44rk4K87/gbwCMR8eWqRUuBc/P35wI/ernrNlIi4tMRMSci5pEd19si4t3A7cCf5cWS2meAiFgHrJJ0ZD7rLcDDJHysc88AJ0iakv97r+x30sc7V+/YLgXem1/tcgKwoaprppiIKNULeBvwGPAE8JnRrs8I7eMbyP4MewC4P3+9jaxP+VbgceDnQNto13WE9v9k4Ob8/aHAPcBK4HvApNGu3wjs7zFAZ368bwJmjYVjDXwe+A3wEPCvwKTUjjfwHbJzBH1kf429v96xBUR2Fd8TwINkVwANa3u+9d/MLBFl63IxM7M6HOhmZolwoJuZJcKBbmaWCAe6mVkiHOhmZolwoJuZJeL/A8RVcmANloeQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTj0M6DZvu2B"
      },
      "source": [
        "#CV Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEyAPDs6dUmN",
        "outputId": "c81a49ca-9ef5-47d3-dbf1-827cc19824a2"
      },
      "source": [
        "# Define per-fold accuracy and loss arrays\r\n",
        "acc_per_fold = []\r\n",
        "loss_per_fold = []\r\n",
        "\r\n",
        "# Define the number of folds to split dataset on\r\n",
        "num_folds = 10\r\n",
        "# Defining the function KFold useful to split the dataset\r\n",
        "kfold = KFold(n_splits = num_folds, shuffle=True)\r\n",
        "\r\n",
        "batch_size = 32\r\n",
        "num_epochs = 50\r\n",
        "\r\n",
        "fold_num = 1\r\n",
        "\r\n",
        "# Define the model and compile\r\n",
        "model = make_model()\r\n",
        "model.compile(optimizer=Adam(lr = 0.001), metrics = 'accuracy', loss='binary_crossentropy')\r\n",
        "# Saving the weights before training as reset before each training\r\n",
        "model.save_weights('reset_model.h5')\r\n",
        "\r\n",
        "for train, test in kfold.split(coefficients, labels):\r\n",
        "  ''' Training the model and test it for every fold created \r\n",
        "      from the total original dataset\r\n",
        "  '''\r\n",
        "  # Reset the model loading weights of untrained model\r\n",
        "  model.load_weights('reset_model.h5')\r\n",
        "  # Fitting the model using every folder but one as training\r\n",
        "  history = model.fit(\r\n",
        "          coefficients[train], labels[train],\r\n",
        "          batch_size = batch_size,\r\n",
        "          verbose=0,\r\n",
        "          epochs = num_epochs)  \r\n",
        "  \r\n",
        "  # Evaluate the efficiency of the model\r\n",
        "  scores = model.evaluate(coefficients[test], labels[test], verbose=0)\r\n",
        "  #Printing the results of the training\r\n",
        "  print('In folder {:d}: {} of {:.4f} - {} of {:.4f}'.format(fold_num,\r\n",
        "                                                          model.metrics_names[0],scores[0],\r\n",
        "                                                          model.metrics_names[1], scores[1] ))\r\n",
        "  \r\n",
        "  '''\r\n",
        "  # Visualize loss and accuracy obtanined during the train for each fold  \r\n",
        "  plt.figure(fold_num)\r\n",
        "  plt.suptitle(f'In fold num. {fold_num}')\r\n",
        "  plt.subplot(1, 2, 1)\r\n",
        "  plt.plot(history.history['loss'])\r\n",
        "  plt.subplot(1, 2, 2)\r\n",
        "  plt.plot(history.history['accuracy'])\r\n",
        "  '''\r\n",
        "  #Putting scores on to the corrisponding list to calculate the mean value ot them at the end\r\n",
        "  acc_per_fold.append(scores[1])\r\n",
        "  loss_per_fold.append(scores[0])\r\n",
        "  fold_num += 1\r\n",
        "\r\n",
        "#Computing and printing average scores\r\n",
        "print('Average scores for all folds:')\r\n",
        "print(f'-- Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\r\n",
        "#Calculare loss error as maximum value error\r\n",
        "err_loss = (max(loss_per_fold)-min(loss_per_fold))/2\r\n",
        "print(f'-- Loss: {np.mean(loss_per_fold)} +/- {err_loss}')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In folder 1: loss of 1.3184 - accuracy of 0.6375\n",
            "In folder 2: loss of 1.0543 - accuracy of 0.6625\n",
            "In folder 3: loss of 2.1596 - accuracy of 0.5250\n",
            "In folder 4: loss of 1.2007 - accuracy of 0.6125\n",
            "In folder 5: loss of 0.7166 - accuracy of 0.7000\n",
            "In folder 6: loss of 1.1404 - accuracy of 0.6875\n",
            "In folder 7: loss of 0.8598 - accuracy of 0.7250\n",
            "In folder 8: loss of 1.1669 - accuracy of 0.6329\n",
            "In folder 9: loss of 1.6109 - accuracy of 0.5823\n",
            "In folder 10: loss of 1.5888 - accuracy of 0.6203\n",
            "Average scores for all folds:\n",
            "-- Accuracy: 0.6385443031787872 (+- 0.05596943584669099)\n",
            "-- Loss: 1.2816384315490723 +/- 0.7214896082878113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeKefnt20lx3"
      },
      "source": [
        ""
      ],
      "execution_count": 96,
      "outputs": []
    }
  ]
}